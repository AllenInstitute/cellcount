{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction with trained model\n",
    "\n",
    "1. Loads a model\n",
    "2. Load weights from a checkpoint\n",
    "3. Load dataset for prediction (patch generation is handled by this class)\n",
    "4. Perform prediction\n",
    "5. Show results on wrt original image and label for both channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "#Add path to parent folder for imports\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.data import Pred_Ai224_RG_Dataset, Pred_Sampler\n",
    "from models.unet import Ai224_RG_UNet\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "#Load model\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "tensor = lambda x: torch.tensor(x).to(dtype=torch.float32).to(device)\n",
    "tensor_ = lambda x: torch.as_tensor(x).to(dtype=torch.float32).to(device)\n",
    "tonumpy = lambda x: x.cpu().detach().numpy()\n",
    "\n",
    "model = Ai224_RG_UNet()\n",
    "ckpt_file = '../../../dat/Ai224_RG_models/CE_wt244_Adam_run_v3_norm/44587_ckpt.pt'\n",
    "checkpoint = torch.load(ckpt_file,map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "model.to(device)\n",
    "print(f'Validation loss was {loss:0.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 260\n",
    "output_size = 172\n",
    "scale = 255.\n",
    "\n",
    "im_path = '../../../dat/raw/Unet_tiles_082020/'\n",
    "lbl_path = '../../../dat/proc/Unet_tiles_082020/'\n",
    "\n",
    "val_files = ['527100_1027993339_0065_tile_9_8_', '527103_1027700130_0060_tile_7_3_','529690_1030079321_0085_tile_8_13_']\n",
    "\n",
    "train_files = ['527100_1027993339_0050_tile_8_3_', '529690_1030079321_0047_tile_4_4_', '527100_1027993339_0070_tile_2_8_', \n",
    "               '529690_1030079321_0062_tile_9_10_', '529690_1030079321_0100_tile_9_10_', '529690_1030079321_0072_tile_5_4_', \n",
    "               '527100_1027993339_0080_tile_2_7_', '527103_1027700130_0045_tile_4_9_', '527103_1027700130_0045_tile_6_3_', \n",
    "               '529690_1030079321_0072_tile_8_2_', '527100_1027993339_0065_tile_2_7_', '527103_1027700130_0060_tile_4_7_', \n",
    "               '527103_1027700130_0045_tile_5_12_', '527100_1027993339_0070_tile_9_7_', '527100_1027993339_0050_tile_3_6_', \n",
    "               '527100_1027993339_0080_tile_8_5_', '529690_1030079321_0062_tile_5_4_']\n",
    "\n",
    "fname = val_files[2]\n",
    "\n",
    "\n",
    "im_path = '/Users/fruity/Desktop/'\n",
    "fname = '0539048282-0097_crop2_'\n",
    "pred_dataset = Pred_Ai224_RG_Dataset(patch_size = patch_size,\n",
    "                             output_size = output_size,\n",
    "                             im_path=im_path,\n",
    "                             fname=fname,\n",
    "                             scale=scale)\n",
    "\n",
    "pred_sampler = Pred_Sampler(dataset=pred_dataset)\n",
    "pred_dataloader = DataLoader(pred_dataset, batch_size=20, shuffle=False,\n",
    "                            sampler=pred_sampler, drop_last=False, pin_memory=True)\n",
    "\n",
    "pred_datagen = iter(pred_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:07<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "#Empty tile for labels (this can have some padding at the ends of each axis):\n",
    "n_labels = 3\n",
    "x_size = pred_dataset.output_size*pred_dataset.n_x_patches\n",
    "y_size = pred_dataset.output_size*pred_dataset.n_y_patches\n",
    "output_size = pred_dataset.output_size\n",
    "pred_g = np.empty(shape=[n_labels,x_size, y_size],dtype=float)\n",
    "pred_r = np.empty(shape=[n_labels,x_size, y_size],dtype=float)\n",
    "\n",
    "#Run model to predict labels\n",
    "model.eval()\n",
    "for i in tqdm(range(len(pred_datagen))):\n",
    "    batch = next(pred_datagen)\n",
    "    xg,xr,_,_ = model(tensor_(batch['im']))\n",
    "    xg = tonumpy(xg)\n",
    "    xr = tonumpy(xr)\n",
    "    #Loop over patches in each batch\n",
    "    for j in range(batch['idx'].shape[0]):\n",
    "        ind = tonumpy(batch['idx'][j])\n",
    "        pred_g[:,ind[0]:ind[0]+output_size,ind[1]:ind[1]+output_size] = xg[j]\n",
    "        pred_r[:,ind[0]:ind[0]+output_size,ind[1]:ind[1]+output_size] = xr[j]\n",
    "\n",
    "#Crop labels to original tile size\n",
    "pred_g = pred_g[:,:pred_dataset.tile_shape_orig[0],:pred_dataset.tile_shape_orig[1]]\n",
    "pred_r = pred_r[:,:pred_dataset.tile_shape_orig[0],:pred_dataset.tile_shape_orig[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aicsimageio import imread\n",
    "get_arr = lambda f: np.squeeze(imread(f))\n",
    "\n",
    "im_g = get_arr(im_path+f'{fname}green.tif')\n",
    "lbl_g = im_g\n",
    "#lbl_g = get_arr(lbl_path+f'{fname}green_labels.tif')\n",
    "\n",
    "im_r = get_arr(im_path+f'{fname}red.tif')\n",
    "lbl_r = im_r\n",
    "#lbl_r = get_arr(lbl_path+f'{fname}red_labels.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt5\n",
    "label = 2\n",
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "plt.subplot(231)\n",
    "plt.imshow(im_g,vmin=0,vmax=50,cmap='gray')\n",
    "ax1 = plt.gca()\n",
    "plt.grid(False)\n",
    "\n",
    "plt.subplot(232,sharex=ax1, sharey=ax1)\n",
    "plt.imshow(np.squeeze(lbl_g),vmin=0,vmax=2)\n",
    "plt.grid(False)\n",
    "\n",
    "plt.subplot(233,sharex=ax1, sharey=ax1)\n",
    "plt.imshow(np.squeeze(pred_g[label,:,:]),vmin=0,vmax=1)\n",
    "plt.grid(False)\n",
    "\n",
    "plt.subplot(234,sharex=ax1, sharey=ax1)\n",
    "plt.imshow(im_r,vmin=0,vmax=10,cmap='gray')\n",
    "plt.grid(False)\n",
    "\n",
    "plt.subplot(235,sharex=ax1, sharey=ax1)\n",
    "plt.imshow(np.squeeze(lbl_r),vmin=0,vmax=2)\n",
    "plt.grid(False)\n",
    "\n",
    "plt.subplot(236,sharex=ax1, sharey=ax1)\n",
    "plt.imshow(np.squeeze(pred_r[label,:,:]),vmin=0,vmax=1)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "log_file='../../../dat/Ai224_RG_models/CE_wt244_Adam_run_v3_norm/log.csv'\n",
    "dat = pd.read_csv(log_file)\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(dat['epoch'],dat['train_ce'],'-b',label='train')\n",
    "plt.plot(dat['epoch'],dat['val_ce'],'-r',alpha=0.1,label='val')\n",
    "ax = plt.gca()\n",
    "ax.set_ylim(1.0, 1.4)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantifying performance\n",
    " - Obtain cell center co-ordinates\n",
    " - True positive rate, False positive rate etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "from utils.analysis import pred_to_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Green channel co-ordinates\n",
    "com,n_elem = pred_to_xy(fg=np.squeeze(pred_g[2,:,:]),bo=np.squeeze(pred_g[1,:,:]),pred_thr=0.5)\n",
    "#com_gt,_ = pred_to_xy(fg=(lbl_g==2).astype(float),bo=(lbl_g==1).astype(float),thr=lbl_thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1052, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Red channel co-ordinates\n",
    "com,n_elem = pred_to_xy(fg=np.squeeze(pred_r[2,:,:]),bo=np.squeeze(pred_r[1,:,:]),pred_thr=0.5,n_elem_thr=10)\n",
    "#com_gt,_ = pred_to_xy(fg=(lbl_r==2).astype(float),bo=(lbl_r==1).astype(float),thr=lbl_thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pairwise_distances(X=com_gt, Y=com, metric='euclidean')\n",
    "ind_tp_in_gt = np.any(d<cell_radius,axis=1)\n",
    "ind_tp_in_pred = np.any(d<cell_radius,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704 out of 813 ground truth cells detected\n",
      "690 out of 717 predictions are correct\n",
      "\n",
      "\n",
      "0.866 TP\n",
      "0.033 FP\n",
      "0.134 FN\n"
     ]
    }
   ],
   "source": [
    "print(f'{np.sum(ind_tp_in_gt):d} out of {com_gt.shape[0]:d} ground truth cells detected')\n",
    "print(f'{np.sum(ind_tp_in_pred):d} out of {com.shape[0]:d} predictions are correct')\n",
    "print('\\n')\n",
    "\n",
    "tp_rate = np.sum(ind_tp_in_gt)/com_gt.shape[0]\n",
    "fp_rate = (com.shape[0]-np.sum(ind_tp_in_pred))/com_gt.shape[0]\n",
    "fn_rate = (com_gt.shape[0]-np.sum(ind_tp_in_gt))/com_gt.shape[0]\n",
    "print(f'{tp_rate:0.3f} TP')\n",
    "print(f'{fp_rate:0.3f} FP')\n",
    "print(f'{fn_rate:0.3f} FN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt5\n",
    "plt.figure()\n",
    "plt.imshow(im_r,vmin=0,vmax=10,cmap='gray')\n",
    "plt.plot(com[~ind_tp_in_pred,1],com[~ind_tp_in_pred,0],'o',color='magenta',markersize=10,markerfacecolor='none',label='Machine only')\n",
    "plt.plot(com_gt[~ind_tp_in_gt,1],com_gt[~ind_tp_in_gt,0],'o',color='deepskyblue',markersize=12,markerfacecolor='none',label='Human only')\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show all annotations\n",
    "%matplotlib qt5\n",
    "plt.figure()\n",
    "plt.imshow(im_g,vmin=0,vmax=20,cmap='gray')\n",
    "plt.plot(com[:,1],com[:,0],'o',color='magenta',markersize=10,markerfacecolor='none',label='All machine')\n",
    "plt.plot(com_gt[:,1],com_gt[:,0],'o',color='deepskyblue',markersize=12,markerfacecolor='none',label='All human')\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
